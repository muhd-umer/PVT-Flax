{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "from typing import Union, Optional, Iterable, Callable, Tuple, List, Any\n",
    "from layers import AdaptiveAveragePool2D, DropPath, to_2tuple\n",
    "import warnings\n",
    "from jax import grad, jit, vmap\n",
    "\n",
    "# For testing\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(nn.Module):\n",
    "    layers: List[nn.Module]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for lyr in self.layers:\n",
    "            x = lyr(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_with_DepthWiseConv(nn.Module):\n",
    "    hidden_features: int = None\n",
    "    out_features: int = None\n",
    "    act: Callable = nn.gelu\n",
    "    drop: float = 0.0\n",
    "    extra_relu: bool = False\n",
    "    trainable: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, feat_size=List[int]):\n",
    "        in_features = x.shape[-1]\n",
    "        out_features = self.out_features or in_features\n",
    "        hidden_features = self.hidden_features or in_features\n",
    "        drop = nn.Dropout(rate=self.drop, deterministic=not self.trainable)\n",
    "\n",
    "        x = nn.Dense(hidden_features)(x)\n",
    "        B, N, C = x.shape\n",
    "        H, W = feat_size\n",
    "        x = x.reshape(B, H, W, C)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(\n",
    "            hidden_features,\n",
    "            kernel_size=(3, 3),\n",
    "            use_bias=True,\n",
    "            feature_group_count=hidden_features,\n",
    "        )(x)\n",
    "        x = x.reshape(B, -1, x.shape[3])\n",
    "        x = self.act(x)\n",
    "        x = drop(x)\n",
    "        x = nn.Dense(out_features)(x)\n",
    "        x = drop(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    dim: int\n",
    "    num_heads: int = 8\n",
    "    sr_ratio: int = 1\n",
    "    linear: bool = False\n",
    "    qkv_bias: bool = True\n",
    "    attn_drop: float = 0.0\n",
    "    proj_drop: float = 0.0\n",
    "    trainable: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, feat_size=List[int]):\n",
    "        assert (\n",
    "            self.dim % self.num_heads == 0\n",
    "        ), f\"Input dim {self.dim} should be dividable by num_heads {self.num_heads}.\"\n",
    "        head_dim = self.dim // self.num_heads\n",
    "        scale = head_dim**-0.5\n",
    "\n",
    "        attn_drop = nn.Dropout(self.attn_drop, deterministic=not self.trainable)\n",
    "        proj = nn.Dense(self.dim)\n",
    "        proj_drop = nn.Dropout(self.proj_drop, deterministic=not self.trainable)\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        H, W = feat_size\n",
    "\n",
    "        q = nn.Dense(self.dim, use_bias=self.qkv_bias)(x)\n",
    "        q = jnp.transpose(q.reshape(B, N, self.num_heads, -1), (0, 2, 1, 3))\n",
    "\n",
    "        if not self.linear:\n",
    "            pool = None\n",
    "            if self.sr_ratio > 1:\n",
    "                sr = nn.Conv(\n",
    "                    self.dim,\n",
    "                    kernel_size=(self.sr_ratio, self.sr_ratio),\n",
    "                    strides=self.sr_ratio,\n",
    "                )\n",
    "                norm = nn.LayerNorm()\n",
    "            else:\n",
    "                sr = None\n",
    "                norm = None\n",
    "            act = None\n",
    "        else:\n",
    "            pool = AdaptiveAveragePool2D(7)\n",
    "            sr = nn.Conv(self.dim, kernel_size=(1, 1), strides=1)\n",
    "            norm = nn.LayerNorm()\n",
    "            act = nn.gelu\n",
    "\n",
    "        if pool is not None:\n",
    "            x_ = x.reshape(B, H, W, C)\n",
    "            x_ = sr((pool(x_))).reshape(B, -1, C)\n",
    "            x_ = norm(x_)\n",
    "            x_ = act(x_)\n",
    "            kv = nn.Dense(self.dim * 2, use_bias=self.qkv_bias)(x_)\n",
    "            kv = jnp.transpose(\n",
    "                kv.reshape(B, -1, 2, self.num_heads, head_dim), (2, 0, 3, 1, 4)\n",
    "            )\n",
    "        else:\n",
    "            if sr is not None:\n",
    "                x_ = x.reshape(B, H, W, C)\n",
    "                x_ = sr(x_).reshape(B, -1, C)\n",
    "                x_ = norm(x_)\n",
    "                kv = nn.Dense(self.dim * 2, use_bias=self.qkv_bias)(x_)\n",
    "                kv = jnp.transpose(\n",
    "                    kv.reshape(B, -1, 2, self.num_heads, head_dim), (2, 0, 3, 1, 4)\n",
    "                )\n",
    "            else:\n",
    "                kv = nn.Dense(self.dim * 2, use_bias=self.qkv_bias)(x)\n",
    "                kv = jnp.transpose(\n",
    "                    kv.reshape(B, -1, 2, self.num_heads, head_dim), (2, 0, 3, 1, 4)\n",
    "                )\n",
    "\n",
    "        k, v = kv[0], kv[1]\n",
    "\n",
    "        attn = (q @ jnp.swapaxes(k, -2, -1)) * scale\n",
    "        attn = nn.softmax(attn, axis=-1)\n",
    "        attn = attn_drop(attn)\n",
    "\n",
    "        x = jnp.swapaxes(attn @ v, 1, 2).reshape(B, N, C)\n",
    "        x = proj(x)\n",
    "        x = proj_drop(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    dim: int\n",
    "    num_heads: int\n",
    "    mlp_ratio: float = 4.0\n",
    "    sr_ratio: int = 1\n",
    "    linear: bool = False\n",
    "    qkv_bias: bool = False\n",
    "    drop: float = 0.0\n",
    "    attn_drop: float = 0.0\n",
    "    drop_path: float = 0.0\n",
    "    act: Callable = nn.gelu\n",
    "    norm_layer: Callable = nn.LayerNorm()\n",
    "    trainable: bool = False\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, feat_size=List[int]):\n",
    "        attn = Attention(\n",
    "            dim=self.dim,\n",
    "            num_heads=self.num_heads,\n",
    "            sr_ratio=self.sr_ratio,\n",
    "            linear=self.linear,\n",
    "            qkv_bias=self.qkv_bias,\n",
    "            attn_drop=self.attn_drop,\n",
    "            proj_drop=self.drop,\n",
    "            trainable=not self.trainable,\n",
    "        )\n",
    "\n",
    "        mlp = MLP_with_DepthWiseConv(\n",
    "            hidden_features=int(self.dim * self.mlp_ratio),\n",
    "            act=self.act,\n",
    "            drop=self.drop,\n",
    "            extra_relu=self.linear,\n",
    "            trainable=not self.trainable,\n",
    "        )\n",
    "\n",
    "        if self.drop_path > 0.0:\n",
    "            drop_path = DropPath(self.drop_path, trainable=not self.trainable)\n",
    "            x = x + drop_path(attn(self.norm_layer(x), feat_size))\n",
    "            x = x + drop_path(mlp(self.norm_layer(x), feat_size))\n",
    "\n",
    "        else:\n",
    "            x = x + attn(self.norm_layer(x), feat_size)\n",
    "            x = x + mlp(self.norm_layer(x), feat_size)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverlapPatchEmbed(nn.Module):\n",
    "    patch_size: int = 7\n",
    "    strides: int = 4\n",
    "    embed_dim: int = 768\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        patch_size = to_2tuple(self.patch_size)\n",
    "        assert (\n",
    "            max(patch_size) > self.strides\n",
    "        ), \"Patch size should be larger than stride.\"\n",
    "        norm = nn.LayerNorm()\n",
    "\n",
    "        x = nn.Conv(\n",
    "            self.embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=self.strides,\n",
    "            padding=(patch_size[0] // 2, patch_size[1] // 2),\n",
    "        )(x)\n",
    "        feat_size = x.shape[1:3]\n",
    "        x = x.reshape(x.shape[0], -1, x.shape[3])\n",
    "        x = norm(x)\n",
    "\n",
    "        return x, feat_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyramidVisionTransformerStage(nn.Module):\n",
    "    dim: int\n",
    "    \n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.zeros((2, 64 * 64, 64))\n",
    "feat_size = [64, 64]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop, key = random.split(random.PRNGKey(0), 2)\n",
    "layer = Block(64, trainable=False)\n",
    "params = layer.init({\"params\": key, \"dropout\": drop}, x, feat_size)[\"params\"]\n",
    "out = layer.apply({\"params\": params}, x, feat_size, rngs={\"dropout\": drop})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('jax-research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7f34a25bdf326343d0e6de4c13ece535d8698d8da540a0e20723ae8e4ddf3e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
